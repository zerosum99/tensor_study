{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = [ 1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_uniform([1],-1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = tf.placeholder(tf.float32, name=\"Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = W * X + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method minimize in module tensorflow.python.training.optimizer:\n",
      "\n",
      "minimize(loss, global_step=None, var_list=None, gate_gradients=1, aggregation_method=None, colocate_gradients_with_ops=False, name=None, grad_loss=None) method of tensorflow.python.training.gradient_descent.GradientDescentOptimizer instance\n",
      "    Add operations to minimize `loss` by updating `var_list`.\n",
      "    \n",
      "    This method simply combines calls `compute_gradients()` and\n",
      "    `apply_gradients()`. If you want to process the gradient before applying\n",
      "    them call `compute_gradients()` and `apply_gradients()` explicitly instead\n",
      "    of using this function.\n",
      "    \n",
      "    Args:\n",
      "      loss: A `Tensor` containing the value to minimize.\n",
      "      global_step: Optional `Variable` to increment by one after the\n",
      "        variables have been updated.\n",
      "      var_list: Optional list or tuple of `Variable` objects to update to\n",
      "        minimize `loss`.  Defaults to the list of variables collected in\n",
      "        the graph under the key `GraphKeys.TRAINABLE_VARIABLES`.\n",
      "      gate_gradients: How to gate the computation of gradients.  Can be\n",
      "        `GATE_NONE`, `GATE_OP`, or  `GATE_GRAPH`.\n",
      "      aggregation_method: Specifies the method used to combine gradient terms.\n",
      "        Valid values are defined in the class `AggregationMethod`.\n",
      "      colocate_gradients_with_ops: If True, try colocating gradients with\n",
      "        the corresponding op.\n",
      "      name: Optional name for the returned operation.\n",
      "      grad_loss: Optional. A `Tensor` holding the gradient computed for `loss`.\n",
      "    \n",
      "    Returns:\n",
      "      An Operation that updates the variables in `var_list`.  If `global_step`\n",
      "      was not `None`, that operation also increments `global_step`.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: If some of the variables are not `Variable` objects.\n",
      "    \n",
      "    @compatibility(eager)\n",
      "    When eager execution is enabled, `loss` should be a Python function that\n",
      "    takes no arguments and computes the value to be minimized. Minimization (and\n",
      "    gradient computation) is done with respect to the elements of `var_list` if\n",
      "    not None, else with respect to any trainable variables created during the\n",
      "    execution of the `loss` function. `gate_gradients`, `aggregation_method`,\n",
      "    `colocate_gradients_with_ops` and `grad_loss` are ignored when eager\n",
      "    execution is enabled.\n",
      "    @end_compatibility\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(optimizer.minimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/dahlmoon/anaconda/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "train_op = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 12.098511 [0.55350924] [1.411303]\n",
      "1 0.40155983 [0.40571272] [1.3076388]\n",
      "2 0.24962787 [0.43732533] [1.2838259]\n",
      "3 0.23618336 [0.448958] [1.2521305]\n",
      "4 0.2249455 [0.46241167] [1.2221212]\n",
      "5 0.21426012 [0.47531226] [1.1927323]\n",
      "6 0.20408268 [0.4879279] [1.164061]\n",
      "7 0.19438857 [0.50023746] [1.1360775]\n",
      "8 0.18515499 [0.5122515] [1.108767]\n",
      "9 0.17635994 [0.52397656] [1.082113]\n",
      "10 0.16798274 [0.5354199] [1.0560998]\n",
      "11 0.16000344 [0.5465881] [1.0307119]\n",
      "12 0.15240319 [0.5574878] [1.0059342]\n",
      "13 0.14516385 [0.5681255] [0.9817523]\n",
      "14 0.13826853 [0.5785075] [0.95815164]\n",
      "15 0.13170065 [0.58863986] [0.9351183]\n",
      "16 0.12544478 [0.5985286] [0.9126387]\n",
      "17 0.11948604 [0.60817975] [0.89069957]\n",
      "18 0.11381037 [0.61759883] [0.8692877]\n",
      "19 0.10840428 [0.6267915] [0.84839064]\n",
      "20 0.103254996 [0.63576317] [0.82799596]\n",
      "21 0.098350346 [0.64451915] [0.80809146]\n",
      "22 0.093678616 [0.65306467] [0.78866553]\n",
      "23 0.08922884 [0.6614048] [0.76970655]\n",
      "24 0.084990405 [0.6695444] [0.7512033]\n",
      "25 0.080953285 [0.6774883] [0.7331449]\n",
      "26 0.077107936 [0.6852413] [0.71552056]\n",
      "27 0.073445216 [0.69280785] [0.6983199]\n",
      "28 0.06995656 [0.7001926] [0.6815328]\n",
      "29 0.06663351 [0.70739967] [0.6651492]\n",
      "30 0.063468374 [0.7144336] [0.6491595]\n",
      "31 0.060453635 [0.72129846] [0.63355416]\n",
      "32 0.057582024 [0.7279982] [0.6183239]\n",
      "33 0.054846805 [0.73453695] [0.60345984]\n",
      "34 0.052241538 [0.7409185] [0.5889531]\n",
      "35 0.049760085 [0.7471467] [0.57479507]\n",
      "36 0.04739641 [0.7532251] [0.56097734]\n",
      "37 0.04514506 [0.7591574] [0.54749185]\n",
      "38 0.04300061 [0.7649471] [0.53433055]\n",
      "39 0.040958058 [0.7705976] [0.52148557]\n",
      "40 0.039012507 [0.77611226] [0.5089494]\n",
      "41 0.037159402 [0.78149444] [0.49671462]\n",
      "42 0.0353943 [0.7867471] [0.4847739]\n",
      "43 0.033713024 [0.7918736] [0.47312027]\n",
      "44 0.032111645 [0.7968768] [0.46174678]\n",
      "45 0.030586338 [0.8017598] [0.45064673]\n",
      "46 0.029133439 [0.8065253] [0.43981346]\n",
      "47 0.027749581 [0.8111763] [0.42924064]\n",
      "48 0.026431462 [0.8157155] [0.41892198]\n",
      "49 0.025175953 [0.8201456] [0.4088514]\n",
      "50 0.023980064 [0.82446915] [0.39902288]\n",
      "51 0.022840997 [0.8286888] [0.38943064]\n",
      "52 0.021756036 [0.83280706] [0.38006902]\n",
      "53 0.020722602 [0.8368262] [0.3709324]\n",
      "54 0.019738285 [0.84074885] [0.36201546]\n",
      "55 0.018800667 [0.8445771] [0.35331282]\n",
      "56 0.017907642 [0.8483134] [0.34481943]\n",
      "57 0.017057018 [0.8519598] [0.33653018]\n",
      "58 0.016246779 [0.8555185] [0.3284402]\n",
      "59 0.0154750645 [0.8589918] [0.32054475]\n",
      "60 0.014739987 [0.8623816] [0.3128391]\n",
      "61 0.014039817 [0.8656898] [0.30531862]\n",
      "62 0.013372916 [0.86891854] [0.29797897]\n",
      "63 0.012737689 [0.87206966] [0.29081577]\n",
      "64 0.012132631 [0.87514496] [0.28382474]\n",
      "65 0.011556326 [0.87814647] [0.27700183]\n",
      "66 0.011007386 [0.8810757] [0.2703429]\n",
      "67 0.01048454 [0.88393456] [0.26384404]\n",
      "68 0.009986519 [0.88672465] [0.2575014]\n",
      "69 0.009512158 [0.88944775] [0.25131124]\n",
      "70 0.0090603065 [0.89210534] [0.2452699]\n",
      "71 0.008629934 [0.89469904] [0.23937377]\n",
      "72 0.00822002 [0.89723045] [0.2336194]\n",
      "73 0.00782955 [0.89970094] [0.22800335]\n",
      "74 0.0074576475 [0.90211207] [0.2225223]\n",
      "75 0.0071034054 [0.9044652] [0.21717301]\n",
      "76 0.0067659817 [0.90676177] [0.21195231]\n",
      "77 0.006444596 [0.9090032] [0.20685714]\n",
      "78 0.0061384733 [0.9111907] [0.20188443]\n",
      "79 0.005846892 [0.9133256] [0.19703127]\n",
      "80 0.0055691637 [0.91540915] [0.19229476]\n",
      "81 0.0053046164 [0.9174427] [0.18767214]\n",
      "82 0.005052653 [0.91942734] [0.18316065]\n",
      "83 0.004812647 [0.92136425] [0.1787576]\n",
      "84 0.0045840396 [0.9232546] [0.17446038]\n",
      "85 0.0043662908 [0.92509943] [0.17026645]\n",
      "86 0.004158893 [0.9269] [0.16617338]\n",
      "87 0.0039613335 [0.9286573] [0.16217868]\n",
      "88 0.0037731712 [0.9303723] [0.15828001]\n",
      "89 0.0035939447 [0.9320461] [0.15447508]\n",
      "90 0.0034232351 [0.93367976] [0.15076163]\n",
      "91 0.0032606313 [0.935274] [0.1471374]\n",
      "92 0.0031057473 [0.93683] [0.14360033]\n",
      "93 0.0029582214 [0.9383486] [0.14014828]\n",
      "94 0.0028177053 [0.9398306] [0.13677919]\n",
      "95 0.0026838554 [0.94127697] [0.1334911]\n",
      "96 0.0025563769 [0.9426887] [0.13028209]\n",
      "97 0.0024349396 [0.9440664] [0.12715018]\n",
      "98 0.0023192887 [0.945411] [0.12409358]\n",
      "99 0.0022091144 [0.9467233] [0.12111045]\n",
      " +++ test +++++\n",
      "[4.854727]\n",
      "[2.4879186]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess :\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(100) :\n",
    "        _, cost_val = sess.run([train_op, cost], feed_dict={X: x_data, Y: y_data})\n",
    "        print(step, cost_val, sess.run(W), sess.run(b))\n",
    "     \n",
    "    print(\" +++ test +++++\")\n",
    "    print(sess.run(hypothesis, feed_dict={X: 5}))\n",
    "    print(sess.run(hypothesis, feed_dict={X: 2.5}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
